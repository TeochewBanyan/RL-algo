{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "core.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "implement actor and critic network\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "PBEOd071cUWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgaLvnndRo20"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.signal\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "If shape is scalar, use shape as the second dimension\n",
        "else extend shape to the later dimensions\n",
        "\"\"\"\n",
        "def combined_shape(length, shape=None):\n",
        "  if shape is None:\n",
        "    return (length, )\n",
        "  return (length, shape) if np.isscalar(shape) else (length, *shape)"
      ],
      "metadata": {
        "id": "69yMXuBhR6gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Pack the network.\n",
        "The first n-2 layers are linear layers, with activation\n",
        "The n-1 layer is output layers, with Identity as config act\n",
        "'sizes' set the layer dimensions\n",
        "nn.Identity: output the same as input\n",
        "\n",
        "\"\"\"\n",
        "def mlp(sizes, activation, output_activation=nn.Identity):\n",
        "  layers = []\n",
        "  for j in range(len(sizes)-1):\n",
        "    act = activation if j<len(sizes)-2 else output_activation\n",
        "    layers+=[nn.Linear(sizes[j], sizes[j+1]), act()]\n",
        "  return nn.Sequential(*layers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7Z14UCRSb_R",
        "outputId": "12087fb8-1668-4261-a8fc-e06e1fa4e837"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 1, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "return the counts of all variables in the module\n",
        "\"\"\"\n",
        "def count_vars(module):\n",
        "  return sum([np.prod(p.shape) for p in module.parameters()])"
      ],
      "metadata": {
        "id": "RrKn-SRhVGF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Policy net\n",
        "Input: state\n",
        "Output: action\n",
        "params:\n",
        "obs_dim, act_dim: state and action space\n",
        "hidden_sizes: sizes of hidden layers\n",
        "activation: activation function for hidden layers\n",
        "act_limit: action space limits\n",
        "\"\"\"\n",
        "class MLPActor(nn.Module):\n",
        "  def __init__(self, obs_dim, act_dim, hidden_sizes, activation, act_limit):\n",
        "    super().__init__()\n",
        "    pi_sizes = [obs_dim] + list(hidden_sizes) + [act_dim]\n",
        "    self.pi = mlp(pi_sizes, activation, nn.Tanh)\n",
        "    self.act_limit = act_limit\n",
        "\n",
        "  def forward(self, obs):\n",
        "    return self.act_limit*self.pi(obs)\n"
      ],
      "metadata": {
        "id": "Lm9p2mm-Vqp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Q network\n",
        "\n",
        "\"\"\"\n",
        "class MLPQFunction(nn.Module):\n",
        "  def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
        "    super().__init__()\n",
        "    self.q = mlp([obs_dim+act_dim] + list(hidden_sizes) + [1], activation)\n",
        "  \"\"\"\n",
        "  params:\n",
        "  obs, act: tensor of state and action\n",
        "  \"\"\"\n",
        "  def forward(self, obs, act):\n",
        "    q = self.q(torch.cat([obs, act], dim=-1))#concat obs and act with the last dim\n",
        "    return torch.squeeze(q, -1)#q should be a scalar"
      ],
      "metadata": {
        "id": "gxQlJe2dXhHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "act_space.high: upper bound\n",
        "\"\"\"\n",
        "class MLPActorCritic(nn.Module):\n",
        "  def __init__(self, obs_space, act_space, hidden_sizes=(256,256), activation=nn.ReLU):\n",
        "    super().__init__()\n",
        "    # init dimension info and action limit \n",
        "    obs_dim = obs_space.shape[0]\n",
        "    act_dim = act_space.shape[0]\n",
        "    act_limit = act_space.high[0]\n",
        "\n",
        "    # init actor and q functions\n",
        "    self.pi = MLPActor(obs_dim, act_dim, hidden_sizes, activation, act_limit)\n",
        "    self.q = MLPQFunction(obs_dim, act_dim, hidden_sizes, activation)\n",
        "  #take action follow the policy network \n",
        "  def act(self, obs):\n",
        "    with torch.no_grad():\n",
        "      return self.pi(obs).numpy()\n"
      ],
      "metadata": {
        "id": "u4RY5AzBaRXG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}