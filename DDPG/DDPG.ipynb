{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DDPG.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBuwe3RkdOc1"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "import gym\n",
        "import time\n",
        "import core\n",
        "\n",
        "from spinup.utils.logx import EpochLogger"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "FIFO buffer\n",
        "\"\"\"\n",
        "class ReplayBuffer:\n",
        "  def __init__(self, obs_dim, act_dim, size):\n",
        "    self.obs_buf = np.zeros(core.combined_shape(size, obs_dim), dtype=np.float32)\n",
        "    self.obs_next_buf = np.zeros(core.combined_shape(size, obs_dim), dtype=np.float32)\n",
        "    self.act_buf = np.zeros(core.combined_shape(size, act_dim), dtype=np.float32)\n",
        "    self.rew_buf = np.zeros(core. dtype=np.float32)\n",
        "    self.done_buf = np.zeros(size, dtype=np.float32)\n",
        "    self.ptr, self.size, self.max_size = 0,0,size\n",
        "\n",
        "  def store(self, obs, act, rew, next_obs, done):\n",
        "    self.obs_buf[self.ptr] = obs\n",
        "    self.obs_next_buf[self.ptr] = next_obs\n",
        "    self.act_buf[self.ptr] = act\n",
        "    self.rew_buf[self.ptr] = rew\n",
        "    self.done_buf = done\n",
        "    self.ptr = (self.ptr+1) % self.max_size\n",
        "    self.size = min(self.max_size, self.size+1)\n",
        "\n",
        "  def sample_batch(self, batch_size=32):\n",
        "    idxs = np.random.randint(0, self.size, size=batch_size)\n",
        "    batch = dict(obs=self.obs_buf[idxs],\n",
        "            obs_next=self.obs_next_buf[idxs],\n",
        "            act=self.act_buf[idxs],\n",
        "            rew=self.rew_buf[idxs],\n",
        "            done=self.done_buf[idxs])\n",
        "    return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in batch.items()}\n",
        "    "
      ],
      "metadata": {
        "id": "AC-xsqc1dr2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "params:\n",
        "  env_fn:\n",
        "  \n",
        "  actor_critic:\n",
        "  \n",
        "  ac_kwargs\n",
        "  \n",
        "  seed: Random number seed\n",
        "  \n",
        "  gamma: Discount factor\n",
        "  \n",
        "  polyak: Interpolation factor in polyak averaging\n",
        "  \n",
        "  pi_lr: learning rate for policy network\n",
        "  \n",
        "  q_lr: learning rate for Q-network\n",
        "  \n",
        "  start_steps: num of steps for uniform-random action\n",
        "  \n",
        "  update_every: num of steps between gradient descent updates\n",
        "\n",
        "  act_noise: Stddev for Gaussian exploration noise added to policy\n",
        "\n",
        "  num_test_episodes: num of episodes to test at the end of epoch\n",
        "\n",
        "  max_ep_len: max len of episode\n",
        "\n",
        "  logger_kwargs: keyword args for EpochLogger\n",
        "\n",
        "  save_freq: freq to save policy and q func\n",
        "\"\"\"\n",
        "def ddpg(env_fn, actor_critic=core.MLPActorCritic, ac_kwargs=dict(), seed=0, steps_per_epoch=4000, \n",
        "         epochs=100, replay_size=int(1e6), gamma=0.99, polyak=0.995, pi_lr=1e-3, q_lr=1e-3, \n",
        "         batch_size=100, start_steps=10000, update_after=1000, update_every=50, act_noise=0.1, \n",
        "         num_test_episodes=10, max_ep_len=1000, logger_kwargs=dict(), save_freq=1):\n",
        "  torch.manual_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "\n",
        "  # load env\n",
        "  env, test_env = env_fn(), env_fn()\n",
        "  obs_dim = env.observation_space.shape\n",
        "  act_dim = env.action_space.shape\n",
        "  # the upper bound of act space\n",
        "  act_limit = env.action_space.high[0]\n",
        "\n",
        "  ac = actor_critic(env.observation_space, env.action_space, **ackwargs)\n",
        "  # target network\n",
        "  ac_targ = deepcopy(ac)\n",
        "\n",
        "  # freeze target network (only update using polyak)\n",
        "  for p in ac_targ.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "  replay_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=replay_size)\n",
        "\n",
        "  # count num of params\n",
        "  var_counts = tuple(core.count_vars(module) for module in [ac.pi, ac.q])\n",
        "\n",
        "  def compute_loss_q(data):\n",
        "    # s, a, r, s', terminal\n",
        "    o, a, r, o2, d = data['obs'], data['act'], data['rew'], data['obs2'], data['done']\n",
        "\n",
        "    q = ac.q(o, a)\n",
        "\n",
        "    # Bellman backup for Q\n",
        "    # Q(s', pi_tar(s'))\n",
        "    with torch.no_grad():\n",
        "      q_pi_targ = ac_targ.q(o2, ac_targ.pi(o2))\n",
        "      backup = r + gamma * (1-d) * q_pi_targ\n",
        "\n",
        "    loss_q = ((q-backup)**2).mean()\n",
        "\n",
        "    return loss_q\n",
        "\n",
        "  # pi: max Q\n",
        "  def compute_loss_pi(data):\n",
        "    o = data['obs']\n",
        "    q_pi = ac.q(o, ac.pi(o))\n",
        "    return -q_pi.mean()\n",
        "\n",
        "  pi_optimizer = Adam(ac.pi.parameters(), lr=pi_lr)\n",
        "  q_optimizer = Adam(ac.q.parameters(), lr=q_lr)\n",
        "\n",
        "  def update(data):\n",
        "    # gradient descent for Q\n",
        "    q_optimizer.zero_grad()\n",
        "    loss_q = compute_loss_q(data)\n",
        "    loss_q.backward()\n",
        "    q_optimizer.step()\n",
        "\n",
        "    # freeze Q-network to save computational effort\n",
        "    for p in ac.q.parameters():\n",
        "      p.requires_grad = False\n",
        "\n",
        "    #gradient des for policy\n",
        "    pi_optimizer.zero_grad()\n",
        "    loss_pi = compute_loss_pi(data)\n",
        "    loss_pi.backward()\n",
        "    pi_optimizer.step()\n",
        "\n",
        "    for p in ac.q.parameters():\n",
        "      p.requires_grad = True\n",
        "\n",
        "    # update target network\n",
        "    with torch.no_grad():\n",
        "      for p,p_targ in zip(ac.parameters(), ac_targ.parameters()):\n",
        "        # use mul_, add_ instead of mul, add (make new tensors)\n",
        "        p_targ.data.nul_(polyak)\n",
        "        p_targ.data.add_((1-polyak)*p.data)\n",
        "  def get_action(o, noise_scale):\n",
        "    a = ac.act(torch.as_tensor(o, dtype=torch.float32))\n",
        "    a += noise_scale * np.random.randn(act_dim)\n",
        "    return np.clip(a, -act_limit, act_limit)\n",
        "\n",
        "  def test_agent():\n",
        "    for j in range(num_test_episodes):\n",
        "      o, d, ep_ret, ep_len = test_env.reset(), False, 0, 0\n",
        "      while not(d or (ep_len==max_ep_len)):\n",
        "        # noise=0 in test\n",
        "        o, r, d, _ = test_env.step(get_action(o, 0))\n",
        "        ep_ret += r\n",
        "        ep_len += 1\n",
        "\n",
        "  total_steps = steps_per_epoch * epochs\n",
        "  start_time = time.time()\n",
        "  o, ep_ret, ep_len = env.reset(), 0, 0\n",
        "\n",
        "  # main loop\n",
        "  for t in range(total_steps):\n",
        "    # randomly sample action at first\n",
        "    if t>start_steps:\n",
        "      a = get_action(o, act_noise)\n",
        "    else:\n",
        "      a = env.action_space.sample()\n",
        "\n",
        "    o2, r, d, _ = env.step(a)\n",
        "    ep_ret += r\n",
        "    ep_len += 1\n",
        "\n",
        "    d = False if ep_len==max_ep_len else d\n",
        "\n",
        "    replay_buffer.store(o, a, r, o2, d)\n",
        "\n",
        "    o = o2\n",
        "\n",
        "    if d or (ep_len==max_ep_len):\n",
        "      o, ep_ret, ep_len = env.reset(), 0, 0\n",
        "\n",
        "    # update\n",
        "    if t >= update_after and t%update_every==0:\n",
        "      for _ in range(update_every):\n",
        "        batch = replay_buffer.sample_batch(batch_size)\n",
        "        update(data=batch)\n",
        "\n",
        "    # End of epoch\n",
        "    if (t+1) % steps_per_epoch ==0:\n",
        "      epoch = (t+1) // steps_per_epoch\n",
        "\n",
        "      if (epoch % save_freq==0) or (epoch==epochs):\n",
        "        logger.save_state({'env':env}, None)\n",
        "\n",
        "      test_agent()"
      ],
      "metadata": {
        "id": "SA2FoitIyHcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  import argparse\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument('--env', type=str, default='HalfCheetah-v2')\n",
        "  parser.add_argument('--hid', type=int, default=256)\n",
        "  parser.add_argument('--l', type=int, default=2)#hidden layers\n",
        "  parser.add_argument('--gamma', type=float, default=0.99)\n",
        "  parser.add_argument('--seed', '-s', type=int, default=0)\n",
        "  parser.add_argument('--epochs', type=int, default=50)\n",
        "  parser.add_argument('--exp_name', type=str, default='ddpg')\n",
        "  args = parser.parse_args()\n",
        "\n",
        "  ddpg(lambda: gym.make(args.env), actor_critic=core.MLPActorCritic, \n",
        "       ac_kwargs=dict(hidden_sizes=[args.hid]*args.l),\n",
        "       gamma=args.gamma, seed=args.seed, epochs=args.epochs)"
      ],
      "metadata": {
        "id": "wEQ9mBE0KaY3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}